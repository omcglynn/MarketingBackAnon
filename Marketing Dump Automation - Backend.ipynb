{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "importStuff"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sqlalchemy\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "cursor = session.connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "getDNMList"
   },
   "outputs": [],
   "source": [
    "SELECT INDEX, FIRST_NAME, LAST_NAME, PHONE, EMAIL, ADDRESS_1, ADDRESS_2, CITY, STATE, ZIPCODE, ZIP_PLUS_FOUR_CODE, INFO_TO_REMOVE FROM SAMPLE_TABLE\n",
    "--SELECT * FROM SAMPLE_TABLE\n",
    "WHERE DUMPED = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "getVar"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Then, we can use the python name to turn cell2 into a Pandas dataframe\n",
    "df = getDNMList.to_pandas()\n",
    "newdf = pd.DataFrame(columns=df.columns)\n",
    "abbreviations = {\n",
    "        \"drive\": [\"dr\", \"drv\", \"dve\", \"dr.\"],\n",
    "        \"street\": [\"st\", \"str\", \"st.\", \"strt\"],\n",
    "        \"avenue\": [\"ave\", \"av\", \"av.\", \"avenu\"],\n",
    "        \"boulevard\": [\"blvd\", \"boul\", \"bvd\", \"boulv\"],\n",
    "        \"road\": [\"rd\", \"rd.\", \"rode\"],\n",
    "        \"lane\": [\"ln\", \"ln.\", \"lne\"],\n",
    "        \"terrace\": [\"ter\", \"ter.\", \"terr\"],\n",
    "        \"place\": [\"pl\", \"pl.\", \"plc\"],\n",
    "        \"court\": [\"ct\", \"ct.\", \"cour\"],\n",
    "        \"south\": ['s'],\n",
    "        'east': ['e'],\n",
    "        'west': ['w'],\n",
    "        'north': ['n'],\n",
    "        'southeast': ['se'],\n",
    "        'southwest': ['sw'],\n",
    "        'northeast': ['ne'],\n",
    "        'northwest': ['nw']\n",
    "    }\n",
    "cursor = session.connection.cursor()\n",
    "\n",
    "#st.write(df.size)\n",
    "    # Create a reverse lookup dictionary to map values back to their keys\n",
    "reverse_abbreviations = {abbr: key for key, abbrs in abbreviations.items() for abbr in abbrs}\n",
    "for x in range(0,int(df.size/12)):\n",
    "    #st.write(x)\n",
    "    # Split the address into parts\n",
    "    add1 = df['ADDRESS_1'][x].lower()\n",
    "    #st.write(add1)\n",
    "    parts = add1.split()\n",
    "    #st.write(parts)\n",
    "    # Create a list to hold possible variations of each part\n",
    "    possible_variations = []\n",
    "\n",
    "    # Iterate over each part of the address\n",
    "    for part in parts:\n",
    "        # Add the original part to the variations\n",
    "        variations = [part]\n",
    "        # If the part has known abbreviations or misspellings, add those to the variations\n",
    "        if part in abbreviations:\n",
    "            variations.extend(abbreviations[part])\n",
    "        elif part in reverse_abbreviations:\n",
    "            # Add the original key and all of its abbreviations/misspellings\n",
    "            key = reverse_abbreviations[part]\n",
    "            variations.append(key)\n",
    "            variations.extend(abbreviations[key])\n",
    "        possible_variations.append(variations)\n",
    "    \n",
    "    # Use itertools.product to create all possible combinations of the variations\n",
    "    all_combinations = list(itertools.product(*possible_variations))\n",
    "    \n",
    "    # Join each combination back into a single string\n",
    "    all_variations = [' '.join(combination) for combination in all_combinations]\n",
    "    for var in all_variations:\n",
    "        cur_row = df.loc[x].copy()\n",
    "        cur_row.transpose()\n",
    "        #st.write(cur_row['ADDRESS_1'])\n",
    "        cur_row['ADDRESS_1'] = var\n",
    "        #st.write(cur_row['ADDRESS_1'])\n",
    "        if var not in df['ADDRESS_1']:\n",
    "            newdf.loc[-1] = cur_row  # adding a row\n",
    "            newdf.index = newdf.index + 1  # shifting index\n",
    "            newdf = newdf.sort_index()\n",
    "    \n",
    "        #st.write(df)\n",
    "    newdf=newdf.drop_duplicates(subset=['ADDRESS_1'], keep='first')\n",
    "\n",
    "    #st.write(newdf)\n",
    "    #st.write(all_variations)\n",
    "#newdf['ID'] = 0\n",
    "#for i in range(0,len(newdf)+1):\n",
    "#    newdf.at[i, 'ID'] = i\n",
    "\n",
    "st.write(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0254125-b74e-4518-b191-d7331d9f1691",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "updateProdDB"
   },
   "outputs": [],
   "source": [
    "updated = []\n",
    "\n",
    "for i in range(1, max(newdf['INDEX'])+1):\n",
    "    for row in newdf.itertuples():\n",
    "        #st.write(row)\n",
    "        #st.write(i)\n",
    "        if row[1] == i:\n",
    "            #st.write(row)\n",
    "            bQuery = f\"\"\"SELECT CUSTOMER_NUMBER, CUSTOMER_FIRST_NAME, CUSTOMER_LAST_NAME FROM SAMPLE_INFO_TABLE \"\"\"\n",
    "            nepaWhere = f\"\"\"WHERE CUSTOMER_FIRST_NAME = upper('{row[2]}') and CUSTOMER_LAST_NAME = upper('{(row[3])}') and CUSTOMER_PHONE_NUMBER = '{row[4]}' and CUSTOMER_EMAIL_ADDRESS = '{row[5]}'\n",
    "                            AND HOUSEHOLD_ADDRESS_1 = upper('{(row[6])}') AND HOUSEHOLD_ADDRESS_2 = upper('{(row[7])}') and HOUSEHOLD_CITY = upper('{(row[8])}') \n",
    "                            and HOUSEHOLD_STATE_CODE = upper('{(row[9])}') and zip_code = '{row[10]}'\"\"\"\n",
    "            neaWhere = f\"\"\"WHERE CUSTOMER_FIRST_NAME = upper('{row[2]}') and CUSTOMER_LAST_NAME = upper('{(row[3])}') and CUSTOMER_EMAIL_ADDRESS = '{row[5]}'\n",
    "                            AND HOUSEHOLD_ADDRESS_1 = upper('{(row[6])}') AND HOUSEHOLD_ADDRESS_2 = upper('{(row[7])}') and HOUSEHOLD_CITY = upper('{(row[8])}') \n",
    "                            and HOUSEHOLD_STATE_CODE = upper('{(row[9])}') and zip_code = '{row[10]}'\"\"\"\n",
    "            npaWhere = f\"\"\"WHERE CUSTOMER_FIRST_NAME = upper('{row[2]}') and CUSTOMER_LAST_NAME = upper('{(row[3])}') and CUSTOMER_PHONE_NUMBER = '{row[4]}'\n",
    "                            AND HOUSEHOLD_ADDRESS_1 = upper('{(row[6])}') AND HOUSEHOLD_ADDRESS_2 = upper('{(row[7])}') and HOUSEHOLD_CITY = upper('{(row[8])}') \n",
    "                            and HOUSEHOLD_STATE_CODE = upper('{(row[9])}') and zip_code = '{row[10]}'\"\"\"\n",
    "            naWhere =f\"\"\"WHERE CUSTOMER_FIRST_NAME = upper('{row[2]}') and CUSTOMER_LAST_NAME = upper('{(row[3])}') \n",
    "                            AND HOUSEHOLD_ADDRESS_1 = upper('{(row[6])}') AND HOUSEHOLD_ADDRESS_2 = upper('{(row[7])}') and HOUSEHOLD_CITY = upper('{(row[8])}') \n",
    "                            and HOUSEHOLD_STATE_CODE = upper('{(row[9])}') and zip_code = '{row[10]}'\"\"\"\n",
    "            aWhere = f\"\"\"WHERE HOUSEHOLD_ADDRESS_1 = upper('{(row[6])}') AND HOUSEHOLD_ADDRESS_2 = upper('{(row[7])}') and HOUSEHOLD_CITY = upper('{(row[8])}') \n",
    "                            and HOUSEHOLD_STATE_CODE = upper('{(row[9])}') and zip_code = '{row[10]}'\"\"\"\n",
    "            \n",
    "            final = pd.DataFrame()\n",
    "            found = False\n",
    "\n",
    "            nepaQ = pd.DataFrame(cursor.execute(bQuery + nepaWhere))\n",
    "            if not nepaQ.empty:\n",
    "                st.write(nepaQ)\n",
    "                st.write(\"found using nepa\")\n",
    "                final = nepaQ\n",
    "                found = True\n",
    "                \n",
    "                \n",
    "            neaQ = pd.DataFrame(cursor.execute(bQuery + neaWhere))\n",
    "            if not neaQ.empty and not found:\n",
    "                st.write(neaQ)\n",
    "                st.write(\"found using nea\")\n",
    "                final = neaQ\n",
    "                found = True\n",
    "                \n",
    "                \n",
    "            npaQ = pd.DataFrame(cursor.execute(bQuery + npaWhere))\n",
    "            if not npaQ.empty and not found:\n",
    "                st.write(npaQ)\n",
    "                st.write(\"found using npa\")\n",
    "                final = npaQ\n",
    "                found = True\n",
    "                \n",
    "                \n",
    "            naQ = pd.DataFrame(cursor.execute(bQuery + naWhere))\n",
    "            if not naQ.empty and not found:\n",
    "                st.write(naQ)\n",
    "                st.write(\"found using na\")\n",
    "                final = naQ\n",
    "                found = True\n",
    "                \n",
    "                \n",
    "            aQ = pd.DataFrame(cursor.execute(bQuery + aWhere))\n",
    "            if not aQ.empty and not found:\n",
    "                st.write(aQ)\n",
    "                st.write(\"found using a\")\n",
    "                final = aQ\n",
    "                found = True\n",
    "                \n",
    "            #elif not found:\n",
    "                #st.write(\"Customer not found in DB\")\n",
    "\n",
    "            if found and not final.empty:\n",
    "                updated.append(row[1])\n",
    "                cNum = final[0][0]\n",
    "                st.write(cNum)\n",
    "                cursor.execute(f\"\"\"UPDATE SAMPLE_INFO_TABLE SET \n",
    "                         Address_Suppression_Ind = 'P'\n",
    "                        , Do_Not_Mail_Indicator = 'Y'\n",
    "                        , Customer_Last_Modified_Date = CURRENT_DATE\n",
    "                        , Customer_Last_Modified_Source = 'CPRA'\n",
    "                        , Last_Update_Timestamp = CURRENT_TIMESTAMP\n",
    "                        WHERE Customer_Number IN ({cNum});\n",
    "                        \"\"\")\n",
    "                st.write(str(cNum) + \" updated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e564cc-4a16-4e19-b3f7-11cfb64450ba",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "dumpedDumpDate"
   },
   "outputs": [],
   "source": [
    "for i in updated:\n",
    "    cursor.execute(f\"\"\"UPDATE SAMPLE_TABLE SET \n",
    "                        DUMPED = TRUE,\n",
    "                        DUMPED_DT = GETDATE()\n",
    "                        WHERE index = {i};\n",
    "                        \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
